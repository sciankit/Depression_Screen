{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9e33389-d953-48d2-90e4-1fba67ffa3e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_TOKEN\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35865140-22b4-480a-babd-dc077d4d4fc9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tiktoken\n  Downloading tiktoken-0.12.0-cp312-cp312-manylinux_2_28_aarch64.whl.metadata (6.7 kB)\nRequirement already satisfied: regex>=2022.1.18 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-e9051ff2-7473-45b3-8823-a9f36085f363/lib/python3.12/site-packages (from tiktoken) (2026.2.19)\nRequirement already satisfied: requests>=2.26.0 in /databricks/python3/lib/python3.12/site-packages (from tiktoken) (2.32.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /databricks/python3/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /databricks/python3/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2025.1.31)\nDownloading tiktoken-0.12.0-cp312-cp312-manylinux_2_28_aarch64.whl (1.1 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/1.1 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.1/1.1 MB\u001B[0m \u001B[31m30.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hInstalling collected packages: tiktoken\nSuccessfully installed tiktoken-0.12.0\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2b2f127-42ae-4d51-9808-087db21cff88",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentencepiece\n  Downloading sentencepiece-0.2.1-cp312-cp312-manylinux_2_27_aarch64.manylinux_2_28_aarch64.whl.metadata (10 kB)\nRequirement already satisfied: protobuf in /databricks/python3/lib/python3.12/site-packages (5.29.4)\nDownloading sentencepiece-0.2.1-cp312-cp312-manylinux_2_27_aarch64.manylinux_2_28_aarch64.whl (1.3 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/1.3 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.3/1.3 MB\u001B[0m \u001B[31m27.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hInstalling collected packages: sentencepiece\nSuccessfully installed sentencepiece-0.2.1\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install sentencepiece protobuf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af92f896-f621-4811-a04e-16c2503421d8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Load MentaLLaMA-chat-7B and Run Inference (Corrected)"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "903bad21b2f445cfb2bf2113c5f484d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/291 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-auto_conversion:\nTraceback (most recent call last):\n  File \"/usr/lib/python3.12/threading.py\", line 1073, in _bootstrap_inner\n    self.run()\n  File \"/databricks/python/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 766, in run_closure\n    _threading_Thread_run(self)\n  File \"/usr/lib/python3.12/threading.py\", line 1010, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-8d2bf201-9a0a-49a3-a0df-90d1bf119aa3/lib/python3.12/site-packages/transformers/safetensors_conversion.py\", line 117, in auto_conversion\n    raise e\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-8d2bf201-9a0a-49a3-a0df-90d1bf119aa3/lib/python3.12/site-packages/transformers/safetensors_conversion.py\", line 96, in auto_conversion\n    sha = get_conversion_pr_reference(api, pretrained_model_name_or_path, **cached_file_kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-8d2bf201-9a0a-49a3-a0df-90d1bf119aa3/lib/python3.12/site-packages/transformers/safetensors_conversion.py\", line 72, in get_conversion_pr_reference\n    spawn_conversion(token, private, model_id)\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-8d2bf201-9a0a-49a3-a0df-90d1bf119aa3/lib/python3.12/site-packages/transformers/safetensors_conversion.py\", line 48, in spawn_conversion\n    result = httpx.post(sse_url, follow_redirects=True, json=data).json()\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/databricks/python/lib/python3.12/site-packages/httpx/_models.py\", line 764, in json\n    return jsonlib.loads(self.content, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/json/__init__.py\", line 346, in loads\n    return _default_decoder.decode(s)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/json/decoder.py\", line 337, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/json/decoder.py\", line 355, in raw_decode\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a9cfe2124694ce89dc970d410e5ab03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/183 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post : I fe el an x ious and over wh el med . Question : Pro v ide as u pport iver es ponse and cl in icale x plan ation . <0x0A> I fe el an x ious and over wh el med . <0x0A> <0x0A> Support ive ▁Response : <0x0A> I ▁understand ▁that ▁you ▁are ▁feeling ▁anxious ▁and ▁over wh el med . ▁It ▁can ▁be ▁difficult ▁to ▁manage ▁these ▁emot ions , ▁especially ▁when ▁they ▁are ▁persistent . ▁However , ▁there ▁are ▁some ▁strateg ies ▁that ▁can ▁help . ▁Have ▁you ▁tried ▁pract icing ▁deep ▁breath ing ▁exer cis es ▁or ▁eng aging ▁in ▁activities ▁that ▁bring ▁you ▁joy ▁or ▁relax ation ? ▁These ▁can ▁be ▁helpful ▁in ▁man aging ▁anx iety ▁and ▁over wh elm . ▁Additionally , ▁it ▁may ▁be ▁helpful ▁to ▁reach ▁out ▁to ▁a ▁ther ap ist ▁or ▁coun sel or ▁for ▁additional ▁support . ▁They ▁can ▁provide ▁you ▁with ▁additional ▁cop ing ▁strateg ies ▁and ▁help ▁you ▁work ▁through ▁your ▁emot ions . ▁Remember , ▁you ▁don ' t ▁have ▁to ▁go ▁through ▁this\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model_id = \"klyang/MentaLLaMA-chat-7B\"\n",
    "offload_dir = \"offload\"\n",
    "os.makedirs(offload_dir, exist_ok=True)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "hf_token = os.environ.get(\"HF_TOKEN\", None)\n",
    "\n",
    "# Load tokenizer and model \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, token=hf_token)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    offload_folder=offload_dir,\n",
    "    use_safetensors=False,\n",
    "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
    "    device_map=None,\n",
    "    token=hf_token\n",
    ")\n",
    "\n",
    "def get_clinical_response(user_text):\n",
    "    prompt = f\"Post: {user_text}\\nQuestion: Provide a supportive response and clinical explanation.\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    for k in inputs:\n",
    "        inputs[k] = inputs[k].to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=150)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Example inference\n",
    "print(get_clinical_response(\"I feel anxious and overwhelmed.\"))\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "dependencies": [
     "transformers",
     "torch",
     "accelerate",
     "tiktoken",
     "sentencepiece",
     "protobuf"
    ],
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "MentaLLaMA-",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
