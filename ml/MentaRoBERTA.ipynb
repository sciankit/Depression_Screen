{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af4e2582-f02d-488f-ad42-7eff07d3bb8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee4c4689-a40e-4450-ad89-eeb545231491",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /local_disk0/.ephemeral_nfs/envs/pythonEnv-f9e87c5b-81b4-487d-a159-30fb517f601b/lib/python3.12/site-packages (5.2.0)\nRequirement already satisfied: torch in /local_disk0/.ephemeral_nfs/envs/pythonEnv-f9e87c5b-81b4-487d-a159-30fb517f601b/lib/python3.12/site-packages (2.10.0)\nRequirement already satisfied: accelerate in /local_disk0/.ephemeral_nfs/envs/pythonEnv-f9e87c5b-81b4-487d-a159-30fb517f601b/lib/python3.12/site-packages (1.12.0)\nRequirement already satisfied: huggingface-hub<2.0,>=1.3.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-f9e87c5b-81b4-487d-a159-30fb517f601b/lib/python3.12/site-packages (from transformers) (1.4.1)\nRequirement already satisfied: numpy>=1.17 in /databricks/python3/lib/python3.12/site-packages (from transformers) (2.1.3)\nRequirement already satisfied: packaging>=20.0 in /databricks/python3/lib/python3.12/site-packages (from transformers) (24.1)\nRequirement already satisfied: pyyaml>=5.1 in /databricks/python3/lib/python3.12/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-f9e87c5b-81b4-487d-a159-30fb517f601b/lib/python3.12/site-packages (from transformers) (2026.2.19)\nRequirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-f9e87c5b-81b4-487d-a159-30fb517f601b/lib/python3.12/site-packages (from transformers) (0.22.2)\nRequirement already satisfied: typer-slim in /local_disk0/.ephemeral_nfs/envs/pythonEnv-f9e87c5b-81b4-487d-a159-30fb517f601b/lib/python3.12/site-packages (from transformers) (0.24.0)\nRequirement already satisfied: safetensors>=0.4.3 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-f9e87c5b-81b4-487d-a159-30fb517f601b/lib/python3.12/site-packages (from transformers) (0.7.0)\nRequirement already satisfied: tqdm>=4.27 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-f9e87c5b-81b4-487d-a159-30fb517f601b/lib/python3.12/site-packages (from transformers) (4.67.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /databricks/python3/lib/python3.12/site-packages (from torch) (4.12.2)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (74.0.0)\nRequirement already satisfied: sympy>=1.13.3 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-f9e87c5b-81b4-487d-a159-30fb517f601b/lib/python3.12/site-packages (from torch) (1.14.0)\nRequirement already satisfied: networkx>=2.5.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-f9e87c5b-81b4-487d-a159-30fb517f601b/lib/python3.12/site-packages (from torch) (3.6.1)\nRequirement already satisfied: jinja2 in /databricks/python3/lib/python3.12/site-packages (from torch) (3.1.5)\nRequirement already satisfied: fsspec>=0.8.5 in /databricks/python3/lib/python3.12/site-packages (from torch) (2023.5.0)\nRequirement already satisfied: psutil in /databricks/python3/lib/python3.12/site-packages (from accelerate) (5.9.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-f9e87c5b-81b4-487d-a159-30fb517f601b/lib/python3.12/site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.2.0)\nRequirement already satisfied: httpx<1,>=0.23.0 in /databricks/python3/lib/python3.12/site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (0.27.0)\nRequirement already satisfied: shellingham in /local_disk0/.ephemeral_nfs/envs/pythonEnv-f9e87c5b-81b4-487d-a159-30fb517f601b/lib/python3.12/site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.5.4)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-f9e87c5b-81b4-487d-a159-30fb517f601b/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /databricks/python3/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: typer>=0.24.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-f9e87c5b-81b4-487d-a159-30fb517f601b/lib/python3.12/site-packages (from typer-slim->transformers) (0.24.1)\nRequirement already satisfied: anyio in /databricks/python3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (4.6.2)\nRequirement already satisfied: certifi in /databricks/python3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (2025.1.31)\nRequirement already satisfied: httpcore==1.* in /databricks/python3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (1.0.2)\nRequirement already satisfied: idna in /databricks/python3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (3.7)\nRequirement already satisfied: sniffio in /databricks/python3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (1.3.0)\nRequirement already satisfied: h11<0.15,>=0.13 in /databricks/python3/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (0.14.0)\nRequirement already satisfied: click>=8.2.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-f9e87c5b-81b4-487d-a159-30fb517f601b/lib/python3.12/site-packages (from typer>=0.24.0->typer-slim->transformers) (8.3.1)\nRequirement already satisfied: rich>=12.3.0 in /databricks/python3/lib/python3.12/site-packages (from typer>=0.24.0->typer-slim->transformers) (13.9.4)\nRequirement already satisfied: annotated-doc>=0.0.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-f9e87c5b-81b4-487d-a159-30fb517f601b/lib/python3.12/site-packages (from typer>=0.24.0->typer-slim->transformers) (0.0.4)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /databricks/python3/lib/python3.12/site-packages (from rich>=12.3.0->typer>=0.24.0->typer-slim->transformers) (2.2.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /databricks/python3/lib/python3.12/site-packages (from rich>=12.3.0->typer>=0.24.0->typer-slim->transformers) (2.15.1)\nRequirement already satisfied: mdurl~=0.1 in /databricks/python3/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=12.3.0->typer>=0.24.0->typer-slim->transformers) (0.1.0)\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers torch accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "766bbab2-0403-4ef4-9410-a5ab8ab948e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tried to attach usage logger `pyspark.databricks.pandas.usage_logger`, but an exception was raised: JVM wasn't initialised. Did you call it on executor side?\n/local_disk0/.ephemeral_nfs/envs/pythonEnv-f9e87c5b-81b4-487d-a159-30fb517f601b/lib/python3.12/site-packages/torch/_vmap_internals.py:9: FutureWarning: `isinstance(treespec, LeafSpec)` is deprecated, use `isinstance(treespec, TreeSpec) and treespec.is_leaf()` instead.\n  from torch.utils._pytree import _broadcast_to_and_flatten, tree_flatten, tree_unflatten\nWarning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "367e4aa633ee4cde98edcf88144f7f86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/949 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc450323f7e9460e951078b2afa648df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9acdcf9716a44fffae53eb7e5193c746",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce0482aa50be478b8a96f7caeb736dfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f68ff4b37f3e42e69b65cc2bd21e6401",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69c37e2c9b6740908e06beaba2079875",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f393f697b8554f4b9df450a1cc40d5c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f710c4cbd784bada2a54a3086aa359d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/393 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Configuration\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "MAX_LENGTH = 512\n",
    "REPO = \"SajjadIslam/multiMentalRoBERTA-6-class\"\n",
    "\n",
    "# Initialize Tokenizer and Model\n",
    "tok = AutoTokenizer.from_pretrained(REPO, use_fast=True)\n",
    "mdl = AutoModelForSequenceClassification.from_pretrained(REPO).to(DEVICE).eval()\n",
    "\n",
    "# Map labels from config\n",
    "id2label = {int(k): v for k, v in mdl.config.id2label.items()}\n",
    "\n",
    "def classify_6(text: str):\n",
    "    \"\"\"\n",
    "    Inputs: Sphinx-transcribed text\n",
    "    Outputs: Dictionary with predicted class, confidence, and full probability distribution\n",
    "    \"\"\"\n",
    "    enc = tok(text, truncation=True, padding=\"max_length\", \n",
    "              max_length=MAX_LENGTH, return_tensors=\"pt\").to(DEVICE)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = mdl(**enc).logits\n",
    "        probs = torch.softmax(logits, dim=-1)[0].cpu().numpy()\n",
    "        pid = int(torch.argmax(logits, dim=-1).item())\n",
    "        \n",
    "    return {\n",
    "        \"predicted_class\": id2label[pid],\n",
    "        \"confidence\": float(probs[pid]),\n",
    "        \"probabilities\": {id2label[i]: float(probs[i]) for i in range(len(probs))}\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36c2ae49-736d-42a9-a771-d35aa8f98da4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\nResult: Tier 1 | Class: stress | Conf: 0.9989941716194153\nInput Sample: I feel like my chest is tight and I can't catch my breath, everything is overwhelming.\n\nResult: Tier 2 | Class: suicide | Conf: 0.9342429041862488\nInput Sample: I've lost all hope and I don't see any reason to keep going tomorrow.\n\nResult: Tier 1 | Class: ptsd | Conf: 0.8282128572463989\nInput Sample: I keep having these vivid flashbacks of the accident and I can't sleep.\n\n"
     ]
    }
   ],
   "source": [
    "def classify_and_club(text: str):\n",
    "    result = classify_6(text)\n",
    "    label = result[\"predicted_class\"].lower()\n",
    "    \n",
    "    # Tier mapping logic\n",
    "    if label == \"none\":\n",
    "        risk_tier = 0  # Low Risk (Stable/Passive Monitoring)\n",
    "    elif label == \"suicide\":\n",
    "        risk_tier = 2  # Critical Risk (Immediate Human Bypass)\n",
    "    else:\n",
    "        # Includes: stress, anxiety, depression, ptsd\n",
    "        risk_tier = 1  # Moderate Risk (Trigger ElevenLabs AI Agent)\n",
    "        \n",
    "    result[\"risk_tier\"] = risk_tier\n",
    "    result['text'] = text\n",
    "    return result\n",
    "\n",
    "# 4. Example Run with Different Scenarios\n",
    "examples = [\n",
    "    \"I feel like my chest is tight and I can't catch my breath, everything is overwhelming.\", # Expected: Anxiety/Stress\n",
    "    \"I've lost all hope and I don't see any reason to keep going tomorrow.\", # Expected: Suicidal\n",
    "    \"I keep having these vivid flashbacks of the accident and I can't sleep.\" # Expected: PTSD\n",
    "]\n",
    "\n",
    "print(\"-\" * 30)\n",
    "for ex in examples:\n",
    "    result = classify_and_club(ex)\n",
    "    print(f\"Result: Tier {result['risk_tier']} | Class: {result['predicted_class']} | Conf: {result['confidence']}\")\n",
    "    print(f\"Input Sample: {result['text']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "53323e03-2c91-4a10-a558-f2007dc380cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33cc7f43-05de-4167-bd1b-f46cfa029f70",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;36m  File \u001B[0;32m<command-5314961277940441>, line 15\u001B[0;36m\u001B[0m\n",
       "\u001B[0;31m    return 0, \"Stable: Low Score\"def final_ensemble_decision(phq_score, nlp_result):\u001B[0m\n",
       "\u001B[0m                                 ^\u001B[0m\n",
       "\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m invalid syntax\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "SyntaxError",
        "evalue": "invalid syntax (command-5314961277940441-1813141753, line 15)"
       },
       "metadata": {
        "errorSummary": "<span class='ansi-red-fg'>SyntaxError</span>: invalid syntax (command-5314961277940441-1813141753, line 15)\n[Trace ID: 00-972556f60a3a55979708ecd2b92c818d-31d1c2d48361a18d-00]"
       },
       "removedWidgets": [],
       "sqlProps": {
        "breakingChangeInfo": null,
        "errorClass": "USER_CODE_SYNTAX_ERROR",
        "pysparkCallSite": null,
        "pysparkFragment": null,
        "pysparkSummary": null,
        "sqlState": "KAN02",
        "stackTrace": null,
        "startIndex": null,
        "stopIndex": null
       },
       "stackFrames": [
        "\u001B[0;36m  File \u001B[0;32m<command-5314961277940441>, line 15\u001B[0;36m\u001B[0m\n\u001B[0;31m    return 0, \"Stable: Low Score\"def final_ensemble_decision(phq_score, nlp_result):\u001B[0m\n\u001B[0m                                 ^\u001B[0m\n\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m invalid syntax\n"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_phq_risk(phq_total_score, item_9_positive=False):\n",
    "    \"\"\"\n",
    "    phq_total_score: 0-27\n",
    "    item_9_positive: Boolean (True if user marked any 'better off dead' thoughts)\n",
    "    \"\"\"\n",
    "    # Safety Override: Item 9 is the ultimate redline\n",
    "    if item_9_positive:\n",
    "        return 2, \"Critical: Item 9 Safety Override\"\n",
    "    \n",
    "    if phq_total_score >= 20:\n",
    "        return 2, \"Critical: Severe Score\"\n",
    "    elif phq_total_score >= 10:\n",
    "        return 1, \"Moderate: Elevating Score\"\n",
    "    else:\n",
    "        return 0, \"Stable: Low Score\"\n",
    "    \n",
    "    \n",
    "def final_ensemble_decision(phq_score, nlp_result):\n",
    "    # 1. Start with the Clinical Anchor (PHQ-9)\n",
    "    base_tier, reason = get_phq_risk(phq_score)\n",
    "    \n",
    "    # 2. Integrate multiMentalRoBERTa (Score 1)\n",
    "    # If NLP detects 'suicide', it's an immediate Tier 2 regardless of others\n",
    "    if nlp_result[\"predicted_class\"] == \"suicide\":\n",
    "        return 2, \"Critical: NLP Suicide Detection\"\n",
    "    \n",
    "    # 3. Conflict Resolution Logic\n",
    "    # If PHQ is low (Tier 0) but NLP are both high (Moderate)\n",
    "    # This detects \"Masked Depression\" where self-report is low but signals are high\n",
    "    if base_tier == 0 and nlp_result[\"risk_tier\"] == 1:\n",
    "        return 1, \"Moderate: Passive Signals detect stress mismatch\"\n",
    "        \n",
    "    return base_tier, reason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e002512-486e-42d7-887f-7485a2a2f101",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Define MLflow PythonModel for Mental Health Inference"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local_disk0/.ephemeral_nfs/envs/pythonEnv-9bee46ed-49ab-4608-be50-385d15e78585/lib/python3.12/site-packages/torch/_vmap_internals.py:9: FutureWarning: `isinstance(treespec, LeafSpec)` is deprecated, use `isinstance(treespec, TreeSpec) and treespec.is_leaf()` instead.\n  from torch.utils._pytree import _broadcast_to_and_flatten, tree_flatten, tree_unflatten\n/databricks/python/lib/python3.12/site-packages/mlflow/pyfunc/utils/data_validation.py:186: UserWarning: \u001B[33mAdd type hints to the `predict` method to enable data validation and automatic signature inference during model logging. Check https://mlflow.org/docs/latest/model/python_model.html#type-hint-usage-in-pythonmodel for more details.\u001B[0m\n  color_warning(\n"
     ]
    }
   ],
   "source": [
    "import mlflow.pyfunc\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "class MentalHealthRiskModel(mlflow.pyfunc.PythonModel):\n",
    "    def load_context(self, context):\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.max_length = 512\n",
    "        self.repo = \"SajjadIslam/multiMentalRoBERTA-6-class\"\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.repo, use_fast=True)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(self.repo).to(self.device).eval()\n",
    "        self.id2label = {int(k): v for k, v in self.model.config.id2label.items()}\n",
    "\n",
    "    def classify_6(self, text):\n",
    "        enc = self.tokenizer(text, truncation=True, padding=\"max_length\", \n",
    "                             max_length=self.max_length, return_tensors=\"pt\").to(self.device)\n",
    "        with torch.no_grad():\n",
    "            logits = self.model(**enc).logits\n",
    "            probs = torch.softmax(logits, dim=-1)[0].cpu().numpy()\n",
    "            pid = int(torch.argmax(logits, dim=-1).item())\n",
    "        return {\n",
    "            \"predicted_class\": self.id2label[pid],\n",
    "            \"confidence\": float(probs[pid]),\n",
    "            \"probabilities\": {self.id2label[i]: float(probs[i]) for i in range(len(probs))}\n",
    "        }\n",
    "\n",
    "    def classify_and_club(self, text):\n",
    "        result = self.classify_6(text)\n",
    "        label = result[\"predicted_class\"].lower()\n",
    "        if label == \"none\":\n",
    "            risk_tier = 0\n",
    "        elif label == \"suicide\":\n",
    "            risk_tier = 2\n",
    "        else:\n",
    "            risk_tier = 1\n",
    "        result[\"risk_tier\"] = risk_tier\n",
    "        result['text'] = text\n",
    "        return result\n",
    "\n",
    "    def predict(self, context, model_input):\n",
    "        # model_input: pandas DataFrame with a column 'text'\n",
    "        results = []\n",
    "        for text in model_input['text']:\n",
    "            results.append(self.classify_and_club(text))\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "323bdecc-16b1-4492-bb84-c01f5810e558",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Log Model to MLflow for Serving"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e15f47e718a4590a5231ed24e34346a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/949 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63217591b3124831bde4965032a52332",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fb396a635f0496489c8be0f492ffd92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a13aa2959fd4b5084fbea3e5cb1914e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "581a8abefa1a48568db84c1cce3906f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "656fea9b96094b34a02c529d92ea562e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d02bb85bd8d04b2e888ee30fada9c3ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "466c47bbacd841ba93ec5ba9ab5598c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/393 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/21 19:36:11 INFO mlflow.pyfunc: Inferring model signature from input example\n/databricks/python/lib/python3.12/site-packages/mlflow/types/utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "662b3f7462294dda8ba3b6b3016a4bfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/393 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c86d492e5cd4a56baf7a5843a56c5d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1846b8032364b87a2e84a7247351414",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading /local_disk0/user_tmp_data/spark-9bee46ed-49ab-4608-be50-38/tmp5w2z4xkp/model/python_model.pkl:   0%…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model logged. Go to MLflow Runs to deploy as a REST endpoint.\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import pandas as pd\n",
    "\n",
    "# Instantiate the custom model\n",
    "mental_health_model = MentalHealthRiskModel()\n",
    "\n",
    "# Example input for signature\n",
    "example_input = pd.DataFrame({\"text\": [\"I feel anxious and overwhelmed.\"]})\n",
    "\n",
    "# Log the model\n",
    "with mlflow.start_run():\n",
    "    mlflow.pyfunc.log_model(\n",
    "        artifact_path=\"mental_health_risk_model\",\n",
    "        python_model=mental_health_model,\n",
    "        input_example=example_input,\n",
    "        code_path=None  # If you have extra code files, specify here\n",
    "    )\n",
    "\n",
    "print(\"Model logged. Go to MLflow Runs to deploy as a REST endpoint.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0bfdac1-db20-479d-919c-5a6faa188418",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting databricks-feature-store\n  Downloading databricks_feature_store-0.17.0-py3-none-any.whl.metadata (3.4 kB)\nCollecting databricks-feature-engineering>=0.2.0 (from databricks-feature-store)\n  Downloading databricks_feature_engineering-0.14.0-py3-none-any.whl.metadata (4.3 kB)\nRequirement already satisfied: mlflow-skinny<4,>=2.16.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny[databricks]<4,>=2.16.0->databricks-feature-engineering>=0.2.0->databricks-feature-store) (2.22.0)\nRequirement already satisfied: pyyaml<7,>=6 in /databricks/python3/lib/python3.12/site-packages (from databricks-feature-engineering>=0.2.0->databricks-feature-store) (6.0.2)\nRequirement already satisfied: boto3<2,>=1.16.7 in /databricks/python3/lib/python3.12/site-packages (from databricks-feature-engineering>=0.2.0->databricks-feature-store) (1.36.2)\nCollecting dbl-tempo<1,>=0.1.26 (from databricks-feature-engineering>=0.2.0->databricks-feature-store)\n  Downloading dbl_tempo-0.1.30-py3-none-any.whl.metadata (12 kB)\nCollecting azure-cosmos==4.3.1 (from databricks-feature-engineering>=0.2.0->databricks-feature-store)\n  Downloading azure_cosmos-4.3.1-py3-none-any.whl.metadata (52 kB)\nRequirement already satisfied: numpy<3,>=1.19.2 in /databricks/python3/lib/python3.12/site-packages (from databricks-feature-engineering>=0.2.0->databricks-feature-store) (2.1.3)\nRequirement already satisfied: protobuf<7,>=5 in /databricks/python3/lib/python3.12/site-packages (from databricks-feature-engineering>=0.2.0->databricks-feature-store) (5.29.4)\nCollecting flask<3,>=1.1.2 (from databricks-feature-engineering>=0.2.0->databricks-feature-store)\n  Downloading flask-2.3.3-py3-none-any.whl.metadata (3.6 kB)\nRequirement already satisfied: sqlparse<1,>=0.4.0 in /databricks/python3/lib/python3.12/site-packages (from databricks-feature-engineering>=0.2.0->databricks-feature-store) (0.5.3)\nCollecting databricks-sdk>=0.76.0 (from databricks-feature-engineering>=0.2.0->databricks-feature-store)\n  Downloading databricks_sdk-0.91.0-py3-none-any.whl.metadata (40 kB)\nRequirement already satisfied: azure-core<2.0.0,>=1.23.0 in /databricks/python3/lib/python3.12/site-packages (from azure-cosmos==4.3.1->databricks-feature-engineering>=0.2.0->databricks-feature-store) (1.34.0)\nRequirement already satisfied: botocore<1.37.0,>=1.36.2 in /databricks/python3/lib/python3.12/site-packages (from boto3<2,>=1.16.7->databricks-feature-engineering>=0.2.0->databricks-feature-store) (1.36.3)\nRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /databricks/python3/lib/python3.12/site-packages (from boto3<2,>=1.16.7->databricks-feature-engineering>=0.2.0->databricks-feature-store) (1.0.1)\nRequirement already satisfied: s3transfer<0.12.0,>=0.11.0 in /databricks/python3/lib/python3.12/site-packages (from boto3<2,>=1.16.7->databricks-feature-engineering>=0.2.0->databricks-feature-store) (0.11.3)\nRequirement already satisfied: requests<3,>=2.28.1 in /databricks/python3/lib/python3.12/site-packages (from databricks-sdk>=0.76.0->databricks-feature-engineering>=0.2.0->databricks-feature-store) (2.32.3)\nRequirement already satisfied: google-auth~=2.0 in /databricks/python3/lib/python3.12/site-packages (from databricks-sdk>=0.76.0->databricks-feature-engineering>=0.2.0->databricks-feature-store) (2.40.0)\nCollecting protobuf<7,>=5 (from databricks-feature-engineering>=0.2.0->databricks-feature-store)\n  Downloading protobuf-6.33.5-cp39-abi3-manylinux2014_aarch64.whl.metadata (593 bytes)\nCollecting Werkzeug>=2.3.7 (from flask<3,>=1.1.2->databricks-feature-engineering>=0.2.0->databricks-feature-store)\n  Downloading werkzeug-3.1.6-py3-none-any.whl.metadata (4.0 kB)\nRequirement already satisfied: Jinja2>=3.1.2 in /databricks/python3/lib/python3.12/site-packages (from flask<3,>=1.1.2->databricks-feature-engineering>=0.2.0->databricks-feature-store) (3.1.5)\nCollecting itsdangerous>=2.1.2 (from flask<3,>=1.1.2->databricks-feature-engineering>=0.2.0->databricks-feature-store)\n  Downloading itsdangerous-2.2.0-py3-none-any.whl.metadata (1.9 kB)\nRequirement already satisfied: click>=8.1.3 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-9bee46ed-49ab-4608-be50-385d15e78585/lib/python3.12/site-packages (from flask<3,>=1.1.2->databricks-feature-engineering>=0.2.0->databricks-feature-store) (8.3.1)\nRequirement already satisfied: blinker>=1.6.2 in /usr/lib/python3/dist-packages (from flask<3,>=1.1.2->databricks-feature-engineering>=0.2.0->databricks-feature-store) (1.7.0)\nRequirement already satisfied: cachetools<6,>=5.0.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<4,>=2.16.0->mlflow-skinny[databricks]<4,>=2.16.0->databricks-feature-engineering>=0.2.0->databricks-feature-store) (5.5.1)\nRequirement already satisfied: cloudpickle<4 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<4,>=2.16.0->mlflow-skinny[databricks]<4,>=2.16.0->databricks-feature-engineering>=0.2.0->databricks-feature-store) (3.0.0)\nRequirement already satisfied: fastapi<1 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<4,>=2.16.0->mlflow-skinny[databricks]<4,>=2.16.0->databricks-feature-engineering>=0.2.0->databricks-feature-store) (0.115.12)\nRequirement already satisfied: gitpython<4,>=3.1.9 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<4,>=2.16.0->mlflow-skinny[databricks]<4,>=2.16.0->databricks-feature-engineering>=0.2.0->databricks-feature-store) (3.1.43)\nRequirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<4,>=2.16.0->mlflow-skinny[databricks]<4,>=2.16.0->databricks-feature-engineering>=0.2.0->databricks-feature-store) (6.6.0)\nRequirement already satisfied: opentelemetry-api<3,>=1.9.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<4,>=2.16.0->mlflow-skinny[databricks]<4,>=2.16.0->databricks-feature-engineering>=0.2.0->databricks-feature-store) (1.32.1)\nRequirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<4,>=2.16.0->mlflow-skinny[databricks]<4,>=2.16.0->databricks-feature-engineering>=0.2.0->databricks-feature-store) (1.32.1)\nRequirement already satisfied: packaging<25 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<4,>=2.16.0->mlflow-skinny[databricks]<4,>=2.16.0->databricks-feature-engineering>=0.2.0->databricks-feature-store) (24.1)\nRequirement already satisfied: pydantic<3,>=1.10.8 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<4,>=2.16.0->mlflow-skinny[databricks]<4,>=2.16.0->databricks-feature-engineering>=0.2.0->databricks-feature-store) (2.10.6)\nRequirement already satisfied: typing-extensions<5,>=4.0.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<4,>=2.16.0->mlflow-skinny[databricks]<4,>=2.16.0->databricks-feature-engineering>=0.2.0->databricks-feature-store) (4.12.2)\nRequirement already satisfied: uvicorn<1 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<4,>=2.16.0->mlflow-skinny[databricks]<4,>=2.16.0->databricks-feature-engineering>=0.2.0->databricks-feature-store) (0.34.2)\nRequirement already satisfied: azure-storage-file-datalake>12 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny[databricks]<4,>=2.16.0->databricks-feature-engineering>=0.2.0->databricks-feature-store) (12.17.0)\nRequirement already satisfied: google-cloud-storage>=1.30.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny[databricks]<4,>=2.16.0->databricks-feature-engineering>=0.2.0->databricks-feature-store) (3.1.0)\nRequirement already satisfied: six>=1.11.0 in /usr/lib/python3/dist-packages (from azure-core<2.0.0,>=1.23.0->azure-cosmos==4.3.1->databricks-feature-engineering>=0.2.0->databricks-feature-store) (1.16.0)\nRequirement already satisfied: azure-storage-blob>=12.23.0 in /databricks/python3/lib/python3.12/site-packages (from azure-storage-file-datalake>12->mlflow-skinny[databricks]<4,>=2.16.0->databricks-feature-engineering>=0.2.0->databricks-feature-store) (12.23.0)\nRequirement already satisfied: isodate>=0.6.1 in /databricks/python3/lib/python3.12/site-packages (from azure-storage-file-datalake>12->mlflow-skinny[databricks]<4,>=2.16.0->databricks-feature-engineering>=0.2.0->databricks-feature-store) (0.6.1)\nRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /databricks/python3/lib/python3.12/site-packages (from botocore<1.37.0,>=1.36.2->boto3<2,>=1.16.7->databricks-feature-engineering>=0.2.0->databricks-feature-store) (2.9.0.post0)\nRequirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /databricks/python3/lib/python3.12/site-packages (from botocore<1.37.0,>=1.36.2->boto3<2,>=1.16.7->databricks-feature-engineering>=0.2.0->databricks-feature-store) (2.3.0)\nRequirement already satisfied: starlette<0.47.0,>=0.40.0 in /databricks/python3/lib/python3.12/site-packages (from fastapi<1->mlflow-skinny<4,>=2.16.0->mlflow-skinny[databricks]<4,>=2.16.0->databricks-feature-engineering>=0.2.0->databricks-feature-store) (0.46.2)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /databricks/python3/lib/python3.12/site-packages (from gitpython<4,>=3.1.9->mlflow-skinny<4,>=2.16.0->mlflow-skinny[databricks]<4,>=2.16.0->databricks-feature-engineering>=0.2.0->databricks-feature-store) (4.0.11)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /databricks/python3/lib/python3.12/site-packages (from google-auth~=2.0->databricks-sdk>=0.76.0->databricks-feature-engineering>=0.2.0->databricks-feature-store) (0.2.8)\nRequirement already satisfied: rsa<5,>=3.1.4 in /databricks/python3/lib/python3.12/site-packages (from google-auth~=2.0->databricks-sdk>=0.76.0->databricks-feature-engineering>=0.2.0->databricks-feature-store) (4.9.1)\nRequirement already satisfied: google-api-core<3.0.0dev,>=2.15.0 in /databricks/python3/lib/python3.12/site-packages (from google-cloud-storage>=1.30.0->mlflow-skinny[databricks]<4,>=2.16.0->databricks-feature-engineering>=0.2.0->databricks-feature-store) (2.20.0)\nRequirement already satisfied: google-cloud-core<3.0dev,>=2.4.2 in /databricks/python3/lib/python3.12/site-packages (from google-cloud-storage>=1.30.0->mlflow-skinny[databricks]<4,>=2.16.0->databricks-feature-engineering>=0.2.0->databricks-feature-store) (2.4.3)\nRequirement already satisfied: google-resumable-media>=2.7.2 in /databricks/python3/lib/python3.12/site-packages (from google-cloud-storage>=1.30.0->mlflow-skinny[databricks]<4,>=2.16.0->databricks-feature-engineering>=0.2.0->databricks-feature-store) (2.7.2)\nRequirement already satisfied: google-crc32c<2.0dev,>=1.0 in /databricks/python3/lib/python3.12/site-packages (from google-cloud-storage>=1.30.0->mlflow-skinny[databricks]<4,>=2.16.0->databricks-feature-engineering>=0.2.0->databricks-feature-store) (1.7.1)\nRequirement already satisfied: zipp>=0.5 in /databricks/python3/lib/python3.12/site-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny<4,>=2.16.0->mlflow-skinny[databricks]<4,>=2.16.0->databricks-feature-engineering>=0.2.0->databricks-feature-store) (3.21.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /databricks/python3/lib/python3.12/site-packages (from Jinja2>=3.1.2->flask<3,>=1.1.2->databricks-feature-engineering>=0.2.0->databricks-feature-store) (3.0.2)\nRequirement already satisfied: deprecated>=1.2.6 in /databricks/python3/lib/python3.12/site-packages (from opentelemetry-api<3,>=1.9.0->mlflow-skinny<4,>=2.16.0->mlflow-skinny[databricks]<4,>=2.16.0->databricks-feature-engineering>=0.2.0->databricks-feature-store) (1.2.13)\nRequirement already satisfied: opentelemetry-semantic-conventions==0.53b1 in /databricks/python3/lib/python3.12/site-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny<4,>=2.16.0->mlflow-skinny[databricks]<4,>=2.16.0->databricks-feature-engineering>=0.2.0->databricks-feature-store) (0.53b1)\nRequirement already satisfied: annotated-types>=0.6.0 in /databricks/python3/lib/python3.12/site-packages (from pydantic<3,>=1.10.8->mlflow-skinny<4,>=2.16.0->mlflow-skinny[databricks]<4,>=2.16.0->databricks-feature-engineering>=0.2.0->databricks-feature-store) (0.7.0)\nRequirement already satisfied: pydantic-core==2.27.2 in /databricks/python3/lib/python3.12/site-packages (from pydantic<3,>=1.10.8->mlflow-skinny<4,>=2.16.0->mlflow-skinny[databricks]<4,>=2.16.0->databricks-feature-engineering>=0.2.0->databricks-feature-store) (2.27.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /databricks/python3/lib/python3.12/site-packages (from requests<3,>=2.28.1->databricks-sdk>=0.76.0->databricks-feature-engineering>=0.2.0->databricks-feature-store) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.12/site-packages (from requests<3,>=2.28.1->databricks-sdk>=0.76.0->databricks-feature-engineering>=0.2.0->databricks-feature-store) (3.7)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.12/site-packages (from requests<3,>=2.28.1->databricks-sdk>=0.76.0->databricks-feature-engineering>=0.2.0->databricks-feature-store) (2025.1.31)\nRequirement already satisfied: h11>=0.8 in /databricks/python3/lib/python3.12/site-packages (from uvicorn<1->mlflow-skinny<4,>=2.16.0->mlflow-skinny[databricks]<4,>=2.16.0->databricks-feature-engineering>=0.2.0->databricks-feature-store) (0.14.0)\nRequirement already satisfied: cryptography>=2.1.4 in /databricks/python3/lib/python3.12/site-packages (from azure-storage-blob>=12.23.0->azure-storage-file-datalake>12->mlflow-skinny[databricks]<4,>=2.16.0->databricks-feature-engineering>=0.2.0->databricks-feature-store) (43.0.3)\nRequirement already satisfied: wrapt<2,>=1.10 in /databricks/python3/lib/python3.12/site-packages (from deprecated>=1.2.6->opentelemetry-api<3,>=1.9.0->mlflow-skinny<4,>=2.16.0->mlflow-skinny[databricks]<4,>=2.16.0->databricks-feature-engineering>=0.2.0->databricks-feature-store) (1.17.0)\nRequirement already satisfied: smmap<6,>=3.0.1 in /databricks/python3/lib/python3.12/site-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny<4,>=2.16.0->mlflow-skinny[databricks]<4,>=2.16.0->databricks-feature-engineering>=0.2.0->databricks-feature-store) (5.0.0)\nRequirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /databricks/python3/lib/python3.12/site-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage>=1.30.0->mlflow-skinny[databricks]<4,>=2.16.0->databricks-feature-engineering>=0.2.0->databricks-feature-store) (1.65.0)\nCollecting protobuf<7,>=5 (from databricks-feature-engineering>=0.2.0->databricks-feature-store)\n  Downloading protobuf-5.29.6-cp38-abi3-manylinux2014_aarch64.whl.metadata (592 bytes)\nRequirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /databricks/python3/lib/python3.12/site-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage>=1.30.0->mlflow-skinny[databricks]<4,>=2.16.0->databricks-feature-engineering>=0.2.0->databricks-feature-store) (1.26.1)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /databricks/python3/lib/python3.12/site-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk>=0.76.0->databricks-feature-engineering>=0.2.0->databricks-feature-store) (0.4.8)\nRequirement already satisfied: anyio<5,>=3.6.2 in /databricks/python3/lib/python3.12/site-packages (from starlette<0.47.0,>=0.40.0->fastapi<1->mlflow-skinny<4,>=2.16.0->mlflow-skinny[databricks]<4,>=2.16.0->databricks-feature-engineering>=0.2.0->databricks-feature-store) (4.6.2)\nRequirement already satisfied: sniffio>=1.1 in /databricks/python3/lib/python3.12/site-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi<1->mlflow-skinny<4,>=2.16.0->mlflow-skinny[databricks]<4,>=2.16.0->databricks-feature-engineering>=0.2.0->databricks-feature-store) (1.3.0)\nRequirement already satisfied: cffi>=1.12 in /databricks/python3/lib/python3.12/site-packages (from cryptography>=2.1.4->azure-storage-blob>=12.23.0->azure-storage-file-datalake>12->mlflow-skinny[databricks]<4,>=2.16.0->databricks-feature-engineering>=0.2.0->databricks-feature-store) (1.17.1)\nRequirement already satisfied: pycparser in /databricks/python3/lib/python3.12/site-packages (from cffi>=1.12->cryptography>=2.1.4->azure-storage-blob>=12.23.0->azure-storage-file-datalake>12->mlflow-skinny[databricks]<4,>=2.16.0->databricks-feature-engineering>=0.2.0->databricks-feature-store) (2.21)\nDownloading databricks_feature_store-0.17.0-py3-none-any.whl (4.2 kB)\nDownloading databricks_feature_engineering-0.14.0-py3-none-any.whl (332 kB)\nDownloading azure_cosmos-4.3.1-py3-none-any.whl (222 kB)\nDownloading databricks_sdk-0.91.0-py3-none-any.whl (808 kB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/808.4 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m808.4/808.4 kB\u001B[0m \u001B[31m24.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading dbl_tempo-0.1.30-py3-none-any.whl (41 kB)\nDownloading flask-2.3.3-py3-none-any.whl (96 kB)\nDownloading itsdangerous-2.2.0-py3-none-any.whl (16 kB)\nDownloading werkzeug-3.1.6-py3-none-any.whl (225 kB)\nDownloading protobuf-5.29.6-cp38-abi3-manylinux2014_aarch64.whl (320 kB)\nInstalling collected packages: Werkzeug, protobuf, itsdangerous, dbl-tempo, flask, databricks-sdk, azure-cosmos, databricks-feature-engineering, databricks-feature-store\n  Attempting uninstall: protobuf\n    Found existing installation: protobuf 5.29.4\n    Not uninstalling protobuf at /databricks/python3/lib/python3.12/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-9bee46ed-49ab-4608-be50-385d15e78585\n    Can't uninstall 'protobuf'. No files were found to uninstall.\n  Attempting uninstall: databricks-sdk\n    Found existing installation: databricks-sdk 0.49.0\n    Not uninstalling databricks-sdk at /databricks/python3/lib/python3.12/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-9bee46ed-49ab-4608-be50-385d15e78585\n    Can't uninstall 'databricks-sdk'. No files were found to uninstall.\nSuccessfully installed Werkzeug-3.1.6 azure-cosmos-4.3.1 databricks-feature-engineering-0.14.0 databricks-feature-store-0.17.0 databricks-sdk-0.91.0 dbl-tempo-0.1.30 flask-2.3.3 itsdangerous-2.2.0 protobuf-5.29.6\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install databricks-feature-store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0feee850-1419-4d1c-b0f8-6b8ff3743723",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create Feature Table for Text Input"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/21 19:59:04 INFO databricks.ml_features._compute_client._compute_client: Setting columns ['id'] of table 'workspace.default.text_features' to NOT NULL.\n2026/02/21 19:59:06 INFO databricks.ml_features._compute_client._compute_client: Setting Primary Keys constraint ['id'] on table 'workspace.default.text_features'.\n2026/02/21 19:59:20 INFO databricks.ml_features._compute_client._compute_client: Created feature table 'workspace.default.text_features'.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature table 'text_features' created and registered.\n"
     ]
    }
   ],
   "source": [
    "from databricks.feature_store import FeatureStoreClient\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType\n",
    "\n",
    "# Initialize Feature Store client\n",
    "fs = FeatureStoreClient()\n",
    "\n",
    "# Create SparkSession (already available in Databricks)\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Define schema\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), False),\n",
    "    StructField(\"user_id\", StringType(), False),\n",
    "    StructField(\"text\", StringType(), False),\n",
    "    StructField(\"fitbit_score\", FloatType(), False)\n",
    "])\n",
    "\n",
    "# Example data\n",
    "feature_data = [\n",
    "    (1, \"userA\", \"I feel anxious and overwhelmed.\", 0.75),\n",
    "    (2, \"userB\", \"I'm doing well today.\", 0.20),\n",
    "    (3, \"userC\", \"Sometimes I feel sad.\", 0.55)\n",
    "]\n",
    "\n",
    "# Create PySpark DataFrame\n",
    "feature_df = spark.createDataFrame(feature_data, schema)\n",
    "\n",
    "# Define feature table name (use default schema)\n",
    "feature_table_name = \"text_features\"  # Or \"default.text_features\" if you want to specify schema\n",
    "\n",
    "# Create feature table\n",
    "fs.create_table(\n",
    "    name=feature_table_name,\n",
    "    primary_keys=[\"id\"],\n",
    "    df=feature_df,\n",
    "    description=\"Input text and wearable features for mental health risk model\"\n",
    ")\n",
    "\n",
    "print(f\"Feature table '{feature_table_name}' created and registered.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": "HIGH"
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "dependencies": [
     "transformers",
     "torch",
     "accelerate",
     "databricks-feature-store"
    ],
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "MentaRoBERTA",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}